{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Recurrent Neural Networks\n",
    "### Starter Code\n",
    "* **PyData Bristol - 5th Meetup:** https://www.meetup.com/PyData-Bristol/events/255667468/\n",
    "* **Event URL:** https://www.eventbrite.co.uk/e/intro-to-recurrent-neural-networks-tickets-52401888459\n",
    "* **Date:** Tue 13th November 2018\n",
    "* **Instructor:** John Sandall\n",
    "* **Contact:** john@coefficient.ai / @john_sandall\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab: Build A Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a basic RNN using just numpy. We won't train it for now, we'll instead just get a feeling for how it's working. We'll use input data that has 20 samples, each with two-features, and two time points (t=0 and t=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 2\n",
    "n_samples = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our input data. Here's X at t=0\n",
    "X0 = np.random.randint(low=-10, high=10, size=(n_samples, n_features))\n",
    "X0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly here's X at t=1\n",
    "X1 = np.random.randint(low=-10, high=10, size=(n_samples, n_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also create the weight matrices `Wx` (connecting X to neurons) and `Wy` (connecting output y at t-1 to neurons at time t)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neurons = 3\n",
    "\n",
    "# Connects 2-features to 3-neurons\n",
    "Wx = np.random.randint(low=-5, high=5, size=(n_features, n_neurons))\n",
    "Wx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connects 3-neuron output at time t-1 to 3-neurons at time t (the recurrent weights)\n",
    "Wy = np.random.randint(low=-5, high=5, size=(n_neurons, n_neurons))\n",
    "Wy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll also need the bias\n",
    "b = np.ones(n_neurons)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Exercise: Calculate Y0!\n",
    "> \n",
    "> **Tips**:\n",
    "> - Remember `Y0 = activation(X0*Wx + b)` and `Y1 = activation(X0*Wx + Y0*Wy + b)`\n",
    "> - You'll need `np.matmul()` to do multiply two matrixes.\n",
    "> - You'll need `np.heaviside(some_vector, 0)` for your activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab: Build A Recurrent Neural Network using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's work through a simple example now using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import SimpleRNN, Dense, TimeDistributed\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Keras is using GPU version of TensorFlow\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at 5 time steps, with:\n",
    "- input X has 20 samples and two features\n",
    "- output y is of length 3 (we have three neurons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input format shape for Keras is (sample size, number of time steps, features)\n",
    "n_steps = 5\n",
    "\n",
    "X = np.random.randint(low=-10, high=10, size=(n_samples, n_steps, n_features))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.random.randint(low=-10, high=10, size=(n_samples, n_steps, n_neurons))\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Exercise: Define a simple `Sequential` RNN model using Keras\n",
    "> - The model should contain one layer (`SimpleRNN` with 3 units, and `return_sequences=True`\n",
    "> - Assign it to a variable called `model`\n",
    "> - Use the Keras documentation if you get stuck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your model here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Exercise: Compile & fit the model\n",
    "> - Use MSE loss and `rmsprop` optimizer.\n",
    "> - Fit it to X and y, using 10 epochs and batch size of 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out! We'll generate some new data `X_new` in the same shape as X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll have one sample, so we want it to have shape (1, 5, 2)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This has shape (1, 5, 2)\n",
    "X_new = np.array([\n",
    "    [[1, 0],  # t = 0 (two features)\n",
    "     [0, 1],  # t = 1\n",
    "     [0, 1],  # t = 2\n",
    "     [0, 1],  # t = 3\n",
    "     [0, 1],  # t = 4\n",
    "    ]\n",
    "])\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our RNN is able to predict some outcomes y of length 3, for each time step.\n",
    "model.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Exercise: Predict single value outputs for y (instead of vectors of length 3)\n",
    "> - Within your `Sequential` model, add a fully connected `Dense()` network with `input_dim=1` and `output_dim=1`\n",
    "> - Compile as before\n",
    "> - Fit to the new y provided\n",
    "> - Predict for `X_new` again, confirming that your outputs are a single time series of 5 numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want a newly shaped y to predict, containing 20 samples over 5 time steps, but otherwise scalar output.\n",
    "y = np.random.randint(low=-10, high=10, size=(n_samples, n_steps, 1))\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Exercise: Train a more fully fledged RNN on real data.\n",
    "> - We'll construct an X input with `1` at t=0 and `0` otherwise.\n",
    "> - Our `y` output just has a simple pattern.\n",
    "> - The RNN should be able to learn the relationship between the X pattern, and the corresponding y pattern.\n",
    "> - Re-use your code from before, i.e. a Sequential model containing a SimpleRNN (this time with 50 units), plus a Dense layer with 1 unit and `sigmoid` activation.\n",
    "> - Compile as before, and fit to `x_train` and `y_train` using 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are our sequences. The RNN should learn to predict the\n",
    "# 0.8 and 0.6 correctly because it can remember the 1 in the inputs.\n",
    "x_seed = [1, 0, 0, 0, 0, 0]\n",
    "y_seed = [1, 0.8, 0.6, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's create 1000 identical samples.\n",
    "n_samples = 1000\n",
    "\n",
    "x_train = np.array([[x_seed] * n_samples]).reshape(n_samples, len(x_seed), 1)\n",
    "y_train = np.array([[y_seed] * n_samples]).reshape(n_samples, len(y_seed), 1)\n",
    "\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your model here...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's predict for this x_new\n",
    "x_new = np.array([[[1],[0],[0],[0],[0],[0]]])\n",
    "x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(x_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab: LSTMs and GRUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM, GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Exercise: Try using the LSTM and GRU units from Keras on the previous example. Does it appear to perform any better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Exercise: Try adding some additional components from the example provided [on the Keras docs here](https://keras.io/getting-started/sequential-model-guide/), such as Dropout. How does this improve things?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Suggested \"homework\" exercise: Work through the Keras \"text generation example\" code: https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py\n",
    "> \n",
    "> Try applying this to your own text dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (conda base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
